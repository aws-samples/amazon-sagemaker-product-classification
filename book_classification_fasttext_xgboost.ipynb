{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "142b7278",
   "metadata": {},
   "source": [
    "# A notebook to explore text classification using word embedders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c9d90",
   "metadata": {},
   "source": [
    "In this notebook, I will explore taking a public dataset of books with metadata such as description, title and category/genre. \n",
    "Ill then use a word embedder to vectorize the description and title and then use XGBoost to create a classifier on the category. \n",
    "I will use GenSim's fasttext implementation as the word embedder to vectorize the description and title. \n",
    "I will then repeat this process but using the native FastText implementation and compare the results. \n",
    "I will then host these models on Amazon's SageMaker "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef365e",
   "metadata": {},
   "source": [
    "## Install libraries, initialise variables, download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f51f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gensim==3.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de942d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import FastText\n",
    "from gensim.test.utils import common_texts  # some example sentences\n",
    "from gensim.utils import simple_preprocess\n",
    "print(common_texts[1])\n",
    "print(len(common_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcef92e",
   "metadata": {},
   "source": [
    "gemsim expects the sentences to already be tokenized and pre-processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b50fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import sagemaker\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2733fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SageMaker session & default S3 bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket() # replace with your own bucket if you have one \n",
    "s3 = sagemaker_session.boto_session.resource('s3')\n",
    "\n",
    "\n",
    "prefix_gensim = 'data_gensim_xgb'\n",
    "prefix_fasttext = 'data_fasttext'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c6d3e3",
   "metadata": {},
   "source": [
    "## Get the data into a working format with just the features we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116788fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete file if already exists\n"
    "! rm meta_Books.json\n"
    "# Downloading the book metadata\n",
    "! wget http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles/meta_Books.json.gz\n",
    "# Uncompressing\n",
    "!gzip -d meta_Books.json.gz -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df597516",
   "metadata": {},
   "source": [
    "The filesize is a bit too big, so we can reduce that if the below line by taking a subset of that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc9017",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reducing the dataset \n",
    "! head -n 100000 meta_Books.json > books_train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec245cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "data=pd.read_json('books_train.json', lines=True)\n",
    "#shuffle the data in place\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "# show first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa306c87",
   "metadata": {},
   "source": [
    "We are only interested in a few columns from this dataset, so we will create a dataframe that onyl returns these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac1d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset = data[[\"category\",\"description\", \"title\" ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ace2633",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6858f9d",
   "metadata": {},
   "source": [
    "We will do some analysis of the data we have here to see how the data looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47cde57",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = data_subset.category.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60dc3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "length.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec08a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"cnt_cats\"] = data_subset.category.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804239db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"cnt_desc\"] = data_subset.description.apply(len)\n",
    "data_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e3f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the rows that have no category\n",
    "data_subset = data_subset[data_subset.cnt_cats != 0]\n",
    "data_subset = data_subset[data_subset.cnt_desc != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42498a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c0ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"cat_x2\"] = data_subset[\"category\"].str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a42ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e16ae7",
   "metadata": {},
   "source": [
    "We can see that the category column has an array which is a hierachy classification of the book. We can train our classifer on just one of those, they are all books, so no need to be interested in the first element, but the second element looks more interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7c0103",
   "metadata": {},
   "source": [
    "We just want to clean some of the data as we can see there was some encoding issues whcih we can fix with a \"replace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e7614",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"cat_x2\"] = data_subset[\"cat_x2\"].replace(\"&amp;\", \"&\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74766fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"cat_x2\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68823c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_subset[\"cat_x2\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee299b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset['description_str'] = data_subset['description'].apply(lambda x: ' '.join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249eb299",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44771493",
   "metadata": {},
   "source": [
    "We want to update the category column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e97c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"cat_x2\"] = data_subset[\"cat_x2\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28fc213",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"cat_x2\"].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b9faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"cat_x2_code\"] = data_subset[\"cat_x2\"].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0aca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd639c3",
   "metadata": {},
   "source": [
    "## GenSim requires us to do some cleansing of the data and tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c6f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text): \n",
    "    '''  \n",
    "    This function takes strings containing numbers and returns strings with numbers removed.\n",
    "    '''\n",
    "    return re.sub(r'\\d+', '', text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55d9ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(text):\n",
    "    '''  \n",
    "    This function takes strings containing mentions and returns strings with \n",
    "    mentions (@ and the account name) removed.\n",
    "    Input(string): one tweet, contains mentions\n",
    "    Output(string): one tweet, mentions (@ and the account name mentioned) removed \n",
    "    '''\n",
    "    mentions = re.compile(r'@\\w+ ?')\n",
    "    return mentions.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd69f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mentions(text):\n",
    "    '''\n",
    "    This function takes strings containing mentions and returns strings with \n",
    "    mentions (@ and the account name) extracted into a different element,\n",
    "    and removes the mentions in the original sentence.\n",
    "    Input(string): one sentence, contains mentions\n",
    "    '''\n",
    "    mentions = [i[1:] for i in text.split() if i.startswith(\"@\")]\n",
    "    sentence = re.compile(r'@\\w+ ?').sub(r'', text)\n",
    "    return sentence,mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0629ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c161eae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import string\n",
    "import glob\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d19ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_list = string.punctuation #you can self define list of punctuation to remove here\n",
    "def remove_punctuation(text): \n",
    "    \"\"\"\n",
    "    This function takes strings containing self defined punctuations and returns\n",
    "    strings with punctuations removed.\n",
    "    \"\"\"\n",
    "    translator = str.maketrans('', '', punc_list) \n",
    "    return text.translate(translator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f81315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(text): \n",
    "    '''\n",
    "    This function takes strings containing mentions and returns strings with \n",
    "    whitespaces removed.\n",
    "    '''\n",
    "    return  \" \".join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d26c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    import re\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87035b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a558ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"description_str\"]=data_subset[\"description_str\"].apply(remove_html_tags)\n",
    "data_subset[\"title\"]=data_subset[\"title\"].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57beeb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"description_str\"] = data_subset[\"description_str\"].str.lower()\n",
    "data_subset[\"title\"] = data_subset[\"title\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b205f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"description_str\"]=data_subset[\"description_str\"].apply(remove_whitespace).apply(remove_punctuation).apply(remove_numbers)\n",
    "data_subset[\"title\"]=data_subset[\"title\"].apply(remove_whitespace).apply(remove_punctuation).apply(remove_numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d8012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb1af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "def tokenize_sent(text): \n",
    "    ''' \n",
    "    This function takes strings and returns tokenized words.\n",
    "    '''\n",
    "    word_tokens = word_tokenize(text)  \n",
    "    return word_tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb693fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"description_str_token\"] = data_subset[\"description_str\"].apply(tokenize_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b27c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"title_token\"] = data_subset[\"title\"].apply(tokenize_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5515051",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddc616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e6bd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter = Counter()\n",
    "for word in  [w for sent in data_subset[\"description_str_token\"] for w in sent]:\n",
    "    counter[word] += 1        \n",
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004a9c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#least frequent words\n",
    "counter.most_common()[:-10:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8553e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 10\n",
    "bottom_n = 10\n",
    "stopwords_list |= set([word for (word, count) in counter.most_common(top_n)])\n",
    "stopwords_list |= set([word for (word, count) in counter.most_common()[:-bottom_n:-1]])\n",
    "stopwords_list |= {'thats'}\n",
    "def remove_stopwords(tokenized_text): \n",
    "    '''\n",
    "    This function takes a list of tokenized words from the description and title, removes self-defined stop words from the list,\n",
    "    and returns the list of words with stop words removed\n",
    "    '''\n",
    "    filtered_text = [word for word in tokenized_text if word not in stopwords_list] \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a677fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"description_str_token\"] = data_subset[\"description_str_token\"].apply(remove_stopwords)\n",
    "data_subset[\"title_token\"] = data_subset[\"title_token\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979c20ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eeea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdb5af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3835340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = Speller(lang='en', fast = True)\n",
    "def spelling_correct(tokenized_text):\n",
    "    \"\"\"\n",
    "    This function takes a list of tokenized words from a sentence, spell check every words and returns the \n",
    "    corrected words if applicable. Note that not every wrong spelling words will be identified.\n",
    "    \"\"\"\n",
    "    corrected = [spell(word) for word in tokenized_text] \n",
    "    return corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9579db05",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "data_subset[\"description_str_token\"] = data_subset[\"description_str_token\"].apply(spelling_correct)\n",
    "data_subset[\"title_token\"] = data_subset[\"title_token\"].apply(spelling_correct)\n",
    "\n",
    "print('Spelling corrected! Time elapsed: {} seconds'.format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa8b00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "def detokenize_sent(text): \n",
    "    ''' \n",
    "    This function takes strings and returns tokenized words.\n",
    "    '''\n",
    "    word_detokens = TreebankWordDetokenizer().detokenize(text)\n",
    "    return word_detokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0a8925",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"description_str_detoken\"] = data_subset[\"description_str_token\"].apply(detokenize_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0b6664",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"title_str_detoken\"] = data_subset[\"title_token\"].apply(detokenize_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bb7bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"desc_title_str_detoken\"] = data_subset[\"description_str_detoken\"] + ' ' + data_subset[\"title_str_detoken\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032fba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset['description_str'].replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47163f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a1093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the rows which don't have data\n",
    "data_subset = data_subset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a617a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset = data_subset.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eba329",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset.to_csv('data_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76247a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_subset = pd.read_csv('data_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce06cbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cdd427",
   "metadata": {},
   "source": [
    "### Now data has been cleansed, we are ready to train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc8231c",
   "metadata": {},
   "source": [
    "We will see when we return a sentence in it's vectorized format, we will have an array of 200 items, as that is the size we have choosen, where this is capturing the semantics of the sentence, and that will enable us to compare 2 sentences and see how similar they are for instance, and for this use-case, to be able to train a classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ed062",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gensim = FastText(size=100, window=3, min_count=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6d4f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_desc = data_subset[\"description_str_token\"] + data_subset[\"title_token\"]\n",
    "token_desc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16658a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "model_gensim.build_vocab(sentences=token_desc)\n",
    "\n",
    "print('Build vocab done! Time elapsed: {} seconds'.format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced815db",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "model_gensim.train(sentences=token_desc, total_examples=len(token_desc), epochs=50) \n",
    "print('Model trained! Time elapsed: {} seconds'.format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c90636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "fname = get_tmpfile(\"fasttext.model\")\n",
    "\n",
    "model_gensim.save('books_gensim_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e22f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "description_str_detoken = data_subset[\"description_str_detoken\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509d4877",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_description_str = model_gensim.wv[description_str_detoken]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec48466",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vector_description_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9c050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "description_str_detoken[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af5714",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_description_str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db720f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_description_str = np.split(vector_description_str,len(vector_description_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ed8797",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_description_str[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b2a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_str_detoken = data_subset[\"title_str_detoken\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7098582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_title_str = model_gensim.wv[title_str_detoken]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c618ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vector_title_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03ccaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_title_str.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ef7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_title_str = np.split(vector_title_str,len(vector_title_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96cb572",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_desc_title = np.concatenate((vector_title_str, vector_description_str), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb89bd19",
   "metadata": {},
   "source": [
    "We want to reshape the vector into a 2D with same number of rows and concatenating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1040fc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_vector_title_descr = vector_desc_title.reshape(len(vector_title_str),200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00af6489",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_vector_title_descr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73813ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536eacea",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4eb231",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big_vector_title_descr = pd.DataFrame(data=big_vector_title_descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d349e19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big_vector_title_descr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ea729",
   "metadata": {},
   "source": [
    "Our index on both these DataFrames wont align anymore, so we need to reset the index so we can do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db563352",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a907de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset_2 = pd.concat([data_subset, df_big_vector_title_descr], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f6db7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faea808",
   "metadata": {},
   "source": [
    "### We want to check the count of each of the classes to check for class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c29fd8",
   "metadata": {},
   "source": [
    "With another version of XGBoost, we can supply the weights as a vector as a parameter for the training which will improve the model training to help the model be less bias because of the class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2760229",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset_2['cat_x2_code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f61cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset_2_cat_x2_agg = data_subset_2.groupby(by=['cat_x2_code']).count()['index']\n",
    "print(data_subset_2_cat_x2_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40157bb5",
   "metadata": {},
   "source": [
    "Get the data in the format ready for fasttext too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce515364",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset_2[\"fastText_label\"] = '__label__' + data_subset[\"cat_x2_code\"].astype(str) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa86550",
   "metadata": {},
   "source": [
    "We have our data in a format that we like now, but for the training, we can select a few columns for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194c5166",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data_subset_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0887320d",
   "metadata": {},
   "source": [
    "Might be better to pick the columns, rather than drop so many, lets look at the head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ef6bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dataframe before saving the data as CSV\n",
    "df_gensim_xgb_sampleweight = data_subset_2.drop(columns=['index','category','description','title','cnt_cats','cnt_desc','cat_x2','description_str','description_str_token','description_str_detoken','desc_title_str_detoken','title_str_detoken','title_token','fastText_label'])\n",
    "df_fasttext = data_subset_2[['fastText_label','description_str_detoken', 'title_str_detoken']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b147a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fasttext['token_sentence'] = df_fasttext['description_str_detoken'] + \" \" + df_fasttext['title_str_detoken']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb93bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_fasttext['untoken'] = [' '.join(map(str, l)) for l in df_fasttext['token_sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfd8d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fasttext['full'] = df_fasttext['fastText_label'] + ' ' + df_fasttext['token_sentence'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b949f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fasttext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849241c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gensim_xgb_sampleweight.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61960097",
   "metadata": {},
   "source": [
    "### For this version of XGBoost, we need to supply 3 arguments to the model which are the features, labels and optionally the sample weight which is going to help improve the performance of the model as we have an imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4176c56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_gensim_xgb_sampleweight.drop(['cat_x2_code'], axis=1).values\n",
    "y = df_gensim_xgb_sampleweight['cat_x2_code'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac687af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7737b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(X)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7062bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ca5fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fba528",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsne = pd.DataFrame(tsne_results[:,0], columns=['tsne-2d-one'])\n",
    "df_tsne['tsne-2d-two'] = pd.DataFrame(tsne_results[:,1])\n",
    "df_tsne['y'] = y\n",
    "df_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7f2740",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsne = pd.DataFrame(tsne_results[:,0], columns=['tsne-2d-one'])\n",
    "df_tsne['tsne-2d-two'] = tsne_results[:,1]\n",
    "df_tsne['y'] = y\n",
    "df_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd66e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
    "    hue=\"y\",\n",
    "    palette=sns.color_palette(\"hls\", 33),\n",
    "    data=df_tsne,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f081aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c834c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_30 = PCA(n_components=50)\n",
    "pca_result_30 = pca_30.fit_transform(X)\n",
    "print('Cumulative explained variation for 30 principal components: {}'.format(np.sum(pca_30.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31813806",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=300)\n",
    "tsne_pca_results = tsne.fit_transform(pca_result_30)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c8126",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsne_pca = pd.DataFrame(tsne_pca_results[:,0], columns=['tsne-2d-one'])\n",
    "df_tsne_pca['tsne-2d-two'] = pd.DataFrame(tsne_pca_results[:,1])\n",
    "df_tsne_pca['y'] = y\n",
    "df_tsne_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4981380",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
    "    hue=\"y\",\n",
    "    palette=sns.color_palette(\"hls\", 33),\n",
    "    data=df_tsne_pca,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40035cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
    "yX_train = np.column_stack((y_train, X_train))\n",
    "yX_test = np.column_stack((y_test, X_test))\n",
    "np.savetxt(\"book_gensim_train_v1.csv\", yX_train, delimiter=\",\", fmt='%0.3f')\n",
    "np.savetxt(\"book_gensim_test_v1.csv\", yX_test, delimiter=\",\", fmt='%0.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008badea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the dataset to an S3 bucket\n",
    "input_train = sagemaker_session.upload_data(path='book_gensim_train_v1.csv', key_prefix='%s/data' % prefix_gensim)\n",
    "input_validation = sagemaker_session.upload_data(path='book_gensim_test_v1.csv', key_prefix='%s/data' % prefix_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef54569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sagemaker.inputs import TrainingInput\n",
    "\n",
    "train_data = sagemaker.inputs.TrainingInput(s3_data=input_train,content_type=\"csv\")\n",
    "validation_data = sagemaker.inputs.TrainingInput(s3_data=input_validation,content_type=\"csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7507b88d",
   "metadata": {},
   "source": [
    "In our training script, we have a parser that is expecting the hyper-parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0653109",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "        \"n_estimators\": \"300\", \n",
    "        \"n_jobs\":\"4\",\n",
    "        \"max_depth\":\"10\",\n",
    "#        \"min_child_weight\": \"6\",\n",
    "        \"learning_rate\": \"0.1\", \n",
    "        \"objective\":'multi:softmax', \n",
    "#        \"reg_alpha\": \"10\",\n",
    "        \"gamma\": \"4\"\n",
    "}\n",
    "\n",
    "instance_type = \"ml.m5.2xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc01ccf",
   "metadata": {},
   "source": [
    "Below is our estimator using the XGBoost framework and using our training script which is using another version of the XGB algorithm, not the SageMaker built-in algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c08a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated XGBoost to XGBClassifier https://sagemaker.readthedocs.io/en/stable/frameworks/xgboost/using_xgboost.html#train-a-model-with-open-source-xgboost\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "xgb_estimator = XGBoost(\n",
    "    entry_point=\"train.py\",\n",
    "    hyperparameters=hyperparams,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.4xlarge',\n",
    "    framework_version=\"1.2-1\",\n",
    "    eval_metric=\"merror\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bce774",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "\n",
    "xgb_estimator.fit({'train': train_data, 'validation': validation_data })\n",
    "\n",
    "print('xgb_estimator model trained! Time elapsed: {} seconds'.format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80201e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor_gensim = xgb_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.2xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdc6176",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb_predictor_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ecf5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import NumpyDeserializer\n",
    "csv_serializer = CSVSerializer()\n",
    "np_deserializer = NumpyDeserializer()\n",
    "\n",
    "xgb_predictor_gensim.serializer = csv_serializer\n",
    "xgb_predictor_gensim.deserializer = np_deserializer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7555821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictions_test_xgb_weighted = [ float(xgb_predictor_gensim.predict(x)) for x in X_test]  \n",
    "score = f1_score(y_test,predictions_test_xgb_weighted,labels=np.unique(y),average='micro')\n",
    "\n",
    "print('F1 Score(micro): %.1f' % (score * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9bb35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_predictor_gensim.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ee8f47",
   "metadata": {},
   "source": [
    "### In the next steps, we will use the built-in XGBoost which doesn't allow you to set the weights for the classes and see how the results differ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052141fe",
   "metadata": {},
   "source": [
    "If we use the XGBClassifer, then we are going to need to divide our training data into 3 files, X =features, y=Labels, and W=weights - all the same length. \n",
    "\n",
    "We are going to need to cerate a map to class to add the weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b83aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "container_uri = sagemaker.image_uris.retrieve('xgboost', boto3.Session().region_name, version='1.0-1')\n",
    "\n",
    "# Create the estimator\n",
    "xgb_bi = sagemaker.estimator.Estimator(container_uri,\n",
    "                                    role, \n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m4.4xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix_gensim),\n",
    "                                    sagemaker_session=sagemaker_session)\n",
    "# Set the hyperparameters\n",
    "xgb_bi.set_hyperparameters(eta=0.1,\n",
    "                        max_depth=10,\n",
    "                        gamma=4,\n",
    "                        num_class=len(np.unique(y)),\n",
    "                        alpha=10,\n",
    "                        min_child_weight=6,\n",
    "                        silent=0,\n",
    "                        objective='multi:softmax',\n",
    "                        num_round=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ce4a1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_bi.fit({'train': train_data, 'validation': validation_data })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba53aca0",
   "metadata": {},
   "source": [
    "# We trained our model and now want to test out the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1997b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb_bi.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type='ml.m4.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4610a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a57c489",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.serializer = csv_serializer\n",
    "\n",
    "predictions_test = [ float(xgb_predictor.predict(x).decode('utf-8')) for x in X_test] \n",
    "score = f1_score(y_test,predictions_test,labels=np.unique(y),average='micro')\n",
    "\n",
    "print('F1 Score(micro): %.1f' % (score * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c67f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = data_subset_2.description_str[42]\n",
    "sentence_word_embedding = model_gensim.wv[sentence]\n",
    "class_prediction = xgb_predictor_gensim.predict(sentence_word_embedding)\n",
    "\n",
    "print(class_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac9c94c",
   "metadata": {},
   "source": [
    "All done, you can delete your endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce40f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8a0974",
   "metadata": {},
   "source": [
    "# Next we will test out the FastText native supervised Text classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e918e84e",
   "metadata": {},
   "source": [
    "In this step, we want to see if the native FastText algorithm is able to do the same but with less hard work.\n",
    "With native FastText, you do not need to tokenize your sentences, and you also do not need to pick vector size as a parameter for the mdoel training. \n",
    "This algorithm will do the work for you behind the scenes. \n",
    "What we do need to do though, is get the data in to the required format which means adding a string of \"__label__\" before the label and then we will concatenate that with the description and title into one field and then present that to the algorithm. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5a40b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fasttext.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dcfef8",
   "metadata": {},
   "source": [
    "Taken the same index as our test example above to see if the fasttext algo can make the same prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0deacad",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install fasttext==0.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6010030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b06b312",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_dataset = df_fasttext['full']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db37d686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_fasttext_native, val_fasttext_native = train_test_split(fasttext_dataset, test_size=0.33, random_state=42)\n",
    "\n",
    "train_file_name = 'train_books_fasttext_native.csv'\n",
    "valid_file_name = 'valid_books_fasttext_native.csv'\n",
    "train_fasttext_native.to_csv(train_file_name, index=False, header=False)\n",
    "val_fasttext_native.to_csv(valid_file_name, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f062096",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_native = fasttext.train_supervised(input=train_file_name, lr=0.1, epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d7909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelwordGram = fasttext.train_supervised(input=train_file_name, lr=0.1, epoch=50, wordNgrams=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c87c49a",
   "metadata": {},
   "source": [
    "### We will run a simple test with the validation data, we are returned the precision and recall, and we can play with the hyperparameters to tune this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31b1813",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastText_Precision_Recall = model_native.test(valid_file_name, k=1)\n",
    "print(FastText_Precision_Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd3266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = 2*((FastText_Precision_Recall[1]*FastText_Precision_Recall[2])/(FastText_Precision_Recall[1]+FastText_Precision_Recall[2]))\n",
    "print('F1 Score(micro): %.1f' % (f1_score * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bb0d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_ft= pd.read_csv(valid_file_name)\n",
    "df_valid_ft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a7e75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_sample_validation = data_subset_2['description_str_detoken'] + data_subset_2['title_str_detoken']\n",
    "fasttext_sample_validation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a2fa67",
   "metadata": {},
   "source": [
    "## Test the prediction versus what we got with the xgb classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccca444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_native.predict(fasttext_sample_validation[1], k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac83e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelwordGram.predict(fasttext_sample_validation[1], k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ef50b5",
   "metadata": {},
   "source": [
    "We can host our model on SageMaker. Blazing Text built-in algorithm is compatible with Fasttext's models, so we can upload the fastText model to S3 and then point a SageMaker endpoint configuration to this model, and then deploy our endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c6bccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = \"books_fasttext_native.bin\"\n",
    "model_native.save_model(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e290e31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0b7b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -czvf model.tar.gz books_fasttext_native.bin\n",
    "model_location = sagemaker_session.upload_data(\"model.tar.gz\", bucket=bucket, key_prefix=f\"fasttext/model-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}/output\")\n",
    "!rm books_fasttext_native.tar.gz books_fasttext_native.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae09c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.image_uris.retrieve(\"blazingtext\",boto3.Session().region_name,  \"1\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, boto3.Session().region_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0125a385",
   "metadata": {},
   "source": [
    "# Deploy endpoint in SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b27321",
   "metadata": {},
   "source": [
    "Blazing text is compatiable with fasttext models such that you can train the fasttext model wherever you want, and then you can push the model to S3 in the required format, i.e. saved as a .tar.gz file and then can deploy the model in SageMaker to take care of the heavy lifting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe45b083",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use blazing text container and the fasttext model\n",
    "model_fastText_book = sagemaker.Model(\n",
    "    model_data=model_location, \n",
    "    image_uri=container, \n",
    "    role=role, \n",
    "    sagemaker_session=sagemaker_session)\n",
    "\n",
    "#\n",
    "\n",
    "model_fastText_book.deploy(\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = 'ml.m4.xlarge')\n",
    "\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=model_fastText_book.endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6078afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_sample_validation[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd17ac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [ fasttext_sample_validation[1] ]\n",
    "payload = {\"instances\": sentence }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd38837",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictor.predict(payload)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2a7d5b",
   "metadata": {},
   "source": [
    "# Clean up, delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebb7a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fastText_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab54ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
